{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f19443-dcc5-483e-ad2c-800081f5db16",
   "metadata": {},
   "source": [
    "# Homework #2: Simple Neural Network Implementation using Numpy\n",
    "\n",
    "Today we will build a simple neural network from scratch in Python using only the numpy library. We will follow the instructions from the following lin https://iamtrask.github.io/2015/07/12/basic-python-network/k:\n",
    "Basic Python Network\n",
    "(Note: The code in the website is written in Python 2, and not Pytho, try writing the code yourself before reverting to the online examplen \n",
    "#1: Import Librarit numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fb5c4-4f4e-41d5-adc9-95f97bb7950a",
   "metadata": {},
   "source": [
    "### Define the Sigmoid Function and its Derivative\n",
    "- Construct a function returning a sigmoid function:\n",
    "$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "- Construct a function returning the derivative of a sigmoid function:\n",
    "$ \\frac{d\\sigma(x)}{dx} = \\sigma(x)(1 - \\sigma(x)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08594d28-1968-4ce6-bac7-4f42bed6ab41",
   "metadata": {},
   "source": [
    "### Initialize Weights\n",
    "Build an array of three weights (3x1 array â€“ think why these dimensions!) and initialize their value randomly. (It is good practice to use weights with normal distribution of $ \\mu = 0 $ and  $ \\sigma = \\frac{1}{3}  $ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47ab35-6fe8-4fe2-bdda-85d970e810e3",
   "metadata": {},
   "source": [
    "### Training the Neural Network\n",
    "Create a loop, iterating 1000 times (equal to the desired number of learning steps). For each iteration, calculate the difference between the network prediction and the real value of y. Multiply that difference with the sigmoid derivative and use the dot product of this number with the input layer to update your weights for the next iteration.\n",
    "- Input and Output Data Sets\n",
    "``` python\n",
    "X = np.array([[0, 0, 1],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 1]])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [0],\n",
    "              [1],\n",
    "              [1]])on. \t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cec57574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking: We expect to get: [0 0 1 1] We got  [0.19744441 0.19746846 0.82390437 0.82392639] \n",
      " we can see that it is about the same, 0 ~ 0.2, 1 ~ 0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_derivative(sigmoid):\n",
    "    return sigmoid*(1-sigmoid)\n",
    "X = np.array([[0, 0, 1],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 1]])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [0],\n",
    "              [1],\n",
    "              [1]])\n",
    "w = np.random.normal(loc=0, scale=1/3, size=(3,1)) #its shape is like that because the \n",
    "#dimension of the input is 3, and the dimension of the output is 1.\n",
    "b=np.random.normal(loc=0, scale=1/3)\n",
    "learning_rate=0.1\n",
    "total_samples=X.shape[0]\n",
    "for epochs in range(1000):\n",
    "    y_predicted=sigmoid(X@w+b)\n",
    "    w_derivatives=(1/total_samples)*X.T@((y_predicted-y)*sigmoid_derivative(y_predicted))\n",
    "    b_derivative=np.sum((1/total_samples)*((y_predicted-y)*sigmoid_derivative(y_predicted)))\n",
    "    w=w-learning_rate*w_derivatives\n",
    "    b=b-learning_rate*b_derivative\n",
    "\n",
    "print(\"Checking: We expect to get:\", y.T[0], \"We got \",sigmoid(X@w+b).T[0],\"\\n we can see that it is about the same, 0 ~ 0.2, 1 ~ 0.8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2190a6be",
   "metadata": {},
   "source": [
    "Formulas:\n",
    "The prediction\n",
    "$$\n",
    "\\hat{y_i}=\\sigma(x_i^T\\omega+b)\\\\[10pt]\n",
    "$$\n",
    "Where x is a single input (dimension 3), and y is the output(dimension 1) <br>\n",
    "$$\n",
    "L(\\omega,b)=\\frac{1}{2n}\\sum_{i=0}^{n}(\\sigma(x_i \\omega +b)-y_i)^2=\\frac{1}{2n}\\sum_{i=0}^{n}(\\hat{y_i}-y_i)^2 \\\\[10pt]\n",
    "$$\n",
    "Here, X is the matrix of all inputs, and Y is the corresponding labels of them.\n",
    "$$\n",
    "L(\\omega,b)=\\frac{1}{2n}(\\sigma(X \\omega +b)-Y)^2=\\frac{1}{2n}(\\hat{Y}-Y)^2 \\\\[10pt]\n",
    "$$\n",
    "$$\n",
    "\\omega \\in \\mathbb{R^3}, b \\in \\mathbb{R}.\\\\\n",
    "$$\n",
    "$\\omega$ is in dimensions 3x1 beacuase w the input is a vector with a <br>\n",
    "Degree of 3 and the output is in degree of 1, and b is the bias - a scalar.<br>\n",
    "Derivative for any single weight (not bias) $\\omega^l , \\ l = 0,1,..., m$ ($m$ is the number of weights):<br>\n",
    "\n",
    "$$\n",
    "\\nabla _\\omega L(\\omega,b)=\\frac{1}{2n}\\sum_{i=0}^{n}2(\\sigma(x_i \\omega+b)-y_i)\\sigma '(x_i \\omega+b)x_i=\\frac{1}{n}\\sum_{i=0}^{n}(\\sigma(x_i \\omega+b)-y_i)\\sigma(x_i \\omega+b)(1-\\sigma(x_i \\omega+b))x_i \\\\[10pt]\n",
    "$$\n",
    "I will menition, we get this formula, because the derivative of $\\frac{\\partial}{\\partial\\omega^l} \\omega = (0,...,\\omega ^l,...,0)$ or in other words, for $ k \\neq l \\ \\ \\frac{d}{d \\omega _l}\\omega _k =0$ \n",
    "\n",
    "Derivative for the bias: <br>\n",
    "$$\n",
    "\n",
    "\\frac{\\partial}{\\partial b}L(\\omega,b)=\\frac{1}{2n}\\sum_{i=0}^{n}2(\\sigma(x_i \\omega+b)-y_i)\\sigma '(x_i \\omega+b)=\\frac{1}{n}\\sum_{i=0}^{n}(\\sigma(x_i \\omega+b)-y_i)\\sigma(x_i \\omega+b)(1-\\sigma(x_i \\omega+b)) \\\\[10pt]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "\\omega_{k+1}=\\omega_{k}-\\nabla _\\omega L(\\omega,b) \\\\[10pt]\n",
    "$$\n",
    "$$\n",
    "b_{k+1}=b_{k}-\\frac{\\partial}{\\partial b}L(\\omega,b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5cd2d-de64-436e-ae38-3e95c5b04bb7",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "1. Upload the file to your Google Colab or GitHub\n",
    "2. Add your code, run it and test for correcrtness\n",
    "3. Submit a link on moodle to Google Colab or GitHub. Please do not send me the file or a link by mail.\n",
    "4. Make sure to share the link to your notebook with idan.tobis@gmail.com (or make it public)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfc1cd-c9ee-444a-9419-8571d7d8343e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
