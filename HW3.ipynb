{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of UAT for ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theorem\n",
    "Let $f: [a,b] \\to \\mathbb{R}$ to be a continuous function, where $[a,b] \\subset \\mathbb{R} $. \\\n",
    "For any $\\varepsilon >0 $, there exists a feedforward neural network with a single <br> hidden layer and ReLU activation function such that:  <br>\n",
    "$$\n",
    "\\sup \\limits_{x \\in [a,b]} | f(x) - \\hat{f} (x) | < \\varepsilon ,\n",
    "$$\n",
    "where $f \\hat (x)$ in our case is the approximation of the neural network, and <br>\n",
    "$f(x)$ is the function we are approximating.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof\n",
    "## step 1: ReLU properties (as a basis function)\n",
    "A $ReLU$ activation function $Relu: \\mathbb{R} \\to [0, \\infty) $ satisfies:\n",
    "$$\n",
    "\\lim \\limits_{x \\to -\\infty} ReLU (x) =0, \\ \\ \\\n",
    "\\lim \\limits_{x \\to \\infty} ReLU (x) = \\infty\n",
    "$$\n",
    "We can express $ReLU$ as the followings:\n",
    "$$\n",
    "Relu(x)=max(0,x)= \n",
    "\\begin{cases}\n",
    "x,& \\text{if} & x>0, \\\\\n",
    "0,& \\text{if} & x \\leq 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: Function Decomposition\n",
    "Let $ f(x)$ be a continuous function defined on a compact interaval $[a,b]$. \\\n",
    "By the Stone Weierstrass Theorem, $f(x)$ can be uniformly approximated by a finite linear \\\n",
    "combination of basis functions:\n",
    "$$\n",
    "f(x) \\approx \\sum_{i=1}^N c_i \\phi _i(x),\n",
    "$$\n",
    "when $c_i \\in \\mathbb{R}$ is a scalar, and $\\phi _i(x)$ are continuous basis function. \\\n",
    "I will add that $N$ is in our case of neuron networks is the amount of neurons (or the <br> amount of basis  functions), and $\\phi _i$ is $ReLU$ activation function.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3: $ReLU$ Approximates Indicator Function\n",
    "We know we can use indicator functions in the Stone Weierstrass Theorem, \\\n",
    "so I will describe them using $ReLU$ activation function.\n",
    "$$\n",
    "\\chi _{[x_0,x_1]}(x)=\n",
    "\\begin{cases}\n",
    "1,& \\text{if} \\ \\  x \\in [x_0,x_1], \\\\\n",
    "0,& \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$.\n",
    "To describe it using $ReLU$, first we will describe a sigmodial function using $ReLU$, \n",
    "$$\n",
    "y(x)=\n",
    "\\begin{cases}\n",
    "1,& \\text{if} \\ \\  x > 1, \\\\\n",
    "x,& \\text{if} \\ \\  x \\in [0,1], \\\\\n",
    "0,& \\text{if} \\ \\ x<0.  \n",
    "\\end{cases}\n",
    "$$\n",
    "This isn't the reglur sigmoid function $\\sigma(z)=\\frac{1}{1+e^{-z}}$, but a function from its family. <br>\n",
    "I claim that,\n",
    "$$\n",
    "g(x)=y(\\omega x + b)=ReLU(\\omega x + b) - ReLU(\\omega x + b-1) \n",
    "$$\n",
    "#### Proof: \n",
    "First we will write the claim as,\n",
    "$g(x)=y(\\omega x + b)=ReLU(\\omega x + b_1) - ReLU(\\omega x + b_2)$\n",
    "\n",
    "Without loss of generality, assuming $b_1 >b_2$\n",
    "$$\n",
    "ReLU(\\omega x + b_1) - ReLU(\\omega x + b_2) =\n",
    "\\begin{cases}\n",
    "\\omega x + b_1,& \\text{if} & \\omega x + b_1>0, \\\\\n",
    "0,& \\text{if} & \\omega x + b_1 \\leq 0.\n",
    "\\end{cases}\n",
    "- \n",
    "\\begin{cases}\n",
    "\\omega x + b_2,& \\text{if} & \\omega x + b_2>0, \\\\\n",
    "0,& \\text{if} & \\omega x + b_2 \\leq 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "##### We now have 3 cases: <br>\n",
    "case 1: $\\  \\omega x + b_1 \\leq 0 \\ \\text{and} \\ \\omega x + b_2 \\leq 0  $\n",
    "In this case both $ReLUs$ are 0, thus: \n",
    "$$\n",
    "g(x)=0\n",
    "$$\n",
    "case 2: $\\  \\omega x + b_1 > 0 \\ \\text{and} \\ \\omega x + b_2 \\leq 0  $\n",
    "In this case the first $ReLU$ is 0 and the other is $\\ \\omega x + b_1$ , thus: \n",
    "$$\n",
    "g(x)=\\omega x + b_1\n",
    "$$\n",
    "case 3: $\\  \\omega x + b_1 > 0 \\ \\text{and} \\ \\omega x + b_2 > 0  $\n",
    "In this case both $ReLUs$ are not 0, thus:\n",
    "$$\n",
    "g(x)=\\omega x + b_1 - (\\omega x + b_2) = b_1 - b_2\n",
    "$$\n",
    "I can denote $ b_1 = b, \\ b_2 = b-1 $, then <br>\n",
    "$$\n",
    "g(x)=1\n",
    "$$\n",
    "Therefore we can write:\n",
    "$$\n",
    "g(x)=ReLU(\\omega x + b_1) - ReLU(\\omega x + b_2) =\n",
    "ReLU(\\omega x + b) - ReLU(\\omega x + b -1) = \n",
    "$$\n",
    "$$\n",
    "=\n",
    "\\begin{cases}\n",
    "1,& \\text{if} & x>\\frac{1-b}{\\omega}, \\\\\n",
    "\\omega x + b,& \\text{if} & x \\in (  \\frac{-b}{\\omega},\\frac{1-b}{\\omega}], \\\\\n",
    "0,& \\text{if} & x\\leq \\frac{-b}{\\omega} .\n",
    "\\end{cases}\n",
    "\\iff \n",
    "\\begin{cases}\n",
    "1,& \\text{if} & x>\\frac{1-b}{\\omega} ,\\\\\n",
    "\\omega x + b,& \\text{if} & x \\in [  \\frac{-b}{\\omega},\\frac{1-b}{\\omega}], \\\\\n",
    "0,& \\text{if} & x< \\frac{-b}{\\omega} .\n",
    "\\end{cases}\n",
    "=y(\\omega x + b)\n",
    "$$\n",
    "End of the claim's proof.\n",
    "<br>\n",
    "<br>\n",
    "Now, when we proved the claim, we can say that for large values of $\\omega$ the function <br>\n",
    "$y(\\omega x + b)$ transitions sharply at $x=-\\frac{b}{\\omega}$: <br>\n",
    "\n",
    "$y(\\omega x +b) =ReLU(\\omega x + b) - ReLU(\\omega x + b -1) \\to\n",
    "\\begin{cases}\n",
    "1,& \\text{if} & x>\\frac{-b}{\\omega}, \\\\\n",
    "0,& \\text{if} & x< \\frac{-b}{\\omega}.\n",
    "\\end{cases} \\\\\n",
    "$\n",
    "And with that findings, we can express $\\chi$ with $ReLUs$ functions as wanted in this step.\n",
    "#### Remarks\n",
    "1) The assumption $b_1 , b_2$ is valid with our choice of $b_1,b_2$ because, \n",
    "$$\n",
    "b_1>b_2 \\iff b > b-1 \\iff 1>0.\n",
    "$$\n",
    "2) The case $\\  \\omega x + b_1 \\leq 0 \\ \\text{and} \\ \\omega x + b_2 > 0  $ doesn't exist, \\\n",
    "because of our choice that $b_1>b_2$.\n",
    "3) case 1: $\\  \\omega x + b_1 \\leq 0 \\ \\text{and} \\ \\omega x + b_2 \\leq 0  \\iff \\omega x + b \\leq 0 \\ \\text{and} \\ \\omega x + b-1 \\leq 0 $ <br>$\\iff  x\\leq \\frac{-b}{\\omega}$ <br>\n",
    "case 2: $\\  \\omega x + b_1 > 0 \\ \\text{and} \\ \\omega x + b_2 \\leq 0  \\iff \\omega x + b > 0 \\ \\text{and} \\ \\omega x +b-1 \\leq 0$ <br> $ \\iff \\frac{-b}{\\omega}< x \\leq \\frac{1-b}{\\omega}$ <br>\n",
    "case 3: $\\  \\omega x + b_1 > 0 \\ \\text{and} \\ \\omega x + b_2 > 0  \\iff \\omega x + b > 0 \\ \\text{and} \\ \\omega x + b - 1 > 0 $ <br> $\\iff x>\\frac{1-b}{\\omega}$\n",
    "<br>\n",
    "<br>\n",
    "Source used (page 11 Lemma 3.15) <br>\n",
    "https://math.uchicago.edu/%7Emay/REU2018/REUPapers/Guilhoto.pdf\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Constructing the Neural Network\n",
    "Using a finite sum of $ReLU$ functions, we can approximate $f(x)$ as:\n",
    "$$\n",
    "\\hat{f}(x)=\\sum _{i=1} ^N c_i ReLU(\\omega _i x + b_i)\n",
    "$$,\n",
    "where $\\omega _i , c_i,b_i$ are learnable parameters.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Error Bound\n",
    "We will define the approximation error as:\n",
    "$$\n",
    "E(x)=|f(x)-\\hat{f}(x)|\n",
    "$$\n",
    "By the uniform continuity of $f(x)$ and the compactness of $[a,b]$, for any \\\n",
    "$\\varepsilon>0$, there exist parameters $c_i \\,b_i \\ \\text{and}  \\ \\omega _i$ such that: \n",
    "$$\n",
    "\\sup \\limits_{x \\in [a,b]} E(x)< \\varepsilon\n",
    "$$\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In conclusion, we can use a single layer neural network with $ReLU$ activation function <br>to approximate any continuous function on a compact interval $[a,b]$, up to a wanted margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "1) https://math.uchicago.edu/%7Emay/REU2018/REUPapers/Guilhoto.pdf (used in step 3).\n",
    "2) Idan's proof of UAT for sigmoid in the model ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
